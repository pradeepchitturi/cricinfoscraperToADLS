"""
Match Scraper - Uses Selenium to scrape Cricinfo match data with retry logic
"""
import os
import re
import time
import pandas as pd
from datetime import datetime
from urllib.parse import urlparse
from core.driver_manager import DriverManager
from core.page_navigator import PageNavigator
from core.metadata_extractor import MetadataExtractor
from core.commentary_parser import CommentaryParser
from configs.db_config import save_to_db
from configs.adls_config import save_dataframe_to_adls
import json
from selenium.common.exceptions import TimeoutException, WebDriverException, NoSuchElementException
from utils.logger import setup_logger
from scraping.player_extractor import PlayerExtractor

logger = setup_logger(__name__)


class MatchScraper:
    def __init__(self, url, base_dir="data", page_load_timeout=300, max_retries=3):
        """
        Initialize Match Scraper with Selenium

        Args:
            url: Match URL
            base_dir: Base directory for data storage
            page_load_timeout: Page load timeout in seconds (default: 300 = 5 minutes)
            max_retries: Maximum retry attempts (default: 3)
        """
        self.url = url
        self.base_dir = base_dir
        self.page_load_timeout = page_load_timeout
        self.max_retries = max_retries
        self.player_extractor = PlayerExtractor(schema='raw')

    def get_folder_name(self, metadata):
        """Generate folder name from metadata"""
        match_date_text = metadata.get("Match days", "")
        date_part = "UnknownDate"
        if match_date_text:
            match_date = re.search(r"(\d{1,2} \w+ \d{4})", match_date_text)
            if match_date:
                parsed_date = datetime.strptime(match_date.group(1), "%d %B %Y")
                date_part = parsed_date.strftime("%Y%m%d")

        path_parts = urlparse(self.url).path.split('/')
        if len(path_parts) > 3:
            match_slug = path_parts[3]
        else:
            match_slug = "match"

        folder_name = f"{date_part}_{match_slug}".replace(" ", "_")
        return folder_name

    def format_date(self, date_str):
        """Format date string to YYYYMMDD"""
        try:
            date_obj = datetime.strptime(date_str, '%d %B %Y')
            return date_obj.strftime('%Y%m%d')
        except Exception:
            return "unknown_date"

    def _navigate_with_retry(self, driver, url, description="page"):
        """
        Navigate to URL with retry logic

        Args:
            driver: Selenium WebDriver instance
            url: URL to navigate to
            description: Description for logging

        Raises:
            Exception: If all retries fail
        """
        last_exception = None

        for attempt in range(1, self.max_retries + 1):
            try:
                print(f"    Loading {description} (attempt {attempt}/{self.max_retries})...")
                logger.info(f"Loading {description} - attempt {attempt}")

                driver.get(url)
                time.sleep(8)  # Wait for page to load and render

                print(f"    ✓ {description} loaded successfully")
                logger.info(f"{description} loaded successfully")
                return

            except TimeoutException as e:
                last_exception = e
                print(f"    ✗ Timeout loading {description} on attempt {attempt}")
                logger.warning(f"Timeout loading {description} - attempt {attempt}")

                if attempt < self.max_retries:
                    wait_time = attempt * 10  # Progressive backoff: 10s, 20s, 30s
                    print(f"    Waiting {wait_time}s before retry...")
                    time.sleep(wait_time)

                    # Try to refresh if timeout
                    try:
                        driver.refresh()
                        time.sleep(5)
                    except:
                        pass

            except WebDriverException as e:
                last_exception = e
                print(f"    ✗ WebDriver error on attempt {attempt}: {str(e)[:100]}")
                logger.error(f"WebDriver error - attempt {attempt}: {e}")

                if attempt < self.max_retries:
                    wait_time = attempt * 8
                    print(f"    Waiting {wait_time}s before retry...")
                    time.sleep(wait_time)

        # All retries failed
        error_msg = f"Failed to load {description} after {self.max_retries} attempts: {last_exception}"
        logger.error(error_msg)
        raise Exception(error_msg)

    def scrape(self, match_id):
        """
        Scrape match data with retry logic and proper error handling
        Handles regular innings + Super Overs

        Args:
            match_id: Unique match identifier
        """
        driver = None
        driver_manager = None

        try:
            # Initialize driver with longer timeout
            print(f"    Initializing WebDriver...")
            logger.info("Initializing WebDriver")

            driver_manager = DriverManager(
                headless=False,
                page_load_timeout=self.page_load_timeout,
                implicit_wait=15
            )
            driver = driver_manager.start_driver()

            # Navigate to full scorecard with retry
            self._navigate_with_retry(driver, self.url, "full scorecard")

            # Initialize page navigator
            page_nav = PageNavigator(driver)

            # Scroll to load all content
            print(f"    Scrolling to load content...")
            logger.info("Scrolling full scorecard page")
            page_nav.scroll_full_page(5)
            time.sleep(3)

            # Extract metadata
            print(f"    Extracting metadata...")
            logger.info("Extracting metadata")
            metadata = MetadataExtractor.extract_metadata(driver.page_source, match_id)

            # Handle player replacements
            pattern = re.compile(r".*Replacement$")
            keys_to_merge = [k for k in metadata if pattern.match(k)]
            metadata["player_replacements"] = json.dumps({k: metadata.pop(k) for k in keys_to_merge})

            # Extracting player names
            logger.info("Extracting player rosters...")
            players_df, player_results = self.player_extractor.extract_and_store(
                html_content=driver.page_source,
                match_id=match_id
            )

            if player_results['status'] == 'success':
                logger.info(f"Stored {player_results['total_players']} players")
                for team in player_results['teams']:
                    logger.info(f"  - {team}")
            else:
                logger.warning("Player extraction failed")

            # Navigate to commentary page
            commentary_url = self.url.replace("/full-scorecard", "/ball-by-ball-commentary")
            self._navigate_with_retry(driver, commentary_url, "commentary page")

            # Scroll to load commentary
            print(f"    Scrolling to load commentary...")
            logger.info("Scrolling commentary page")
            page_nav.scroll_full_page()
            time.sleep(3)

            # Get current innings (default view)
            print(f"    Extracting innings data...")
            logger.info("Getting current innings team")
            default_innings = self.get_current_innings_team(driver)
            print(f"    Default innings: {default_innings}")
            logger.info(f"Default innings: {default_innings}")

            # Get all available innings from dropdown
            print(f"    Checking for all innings options...")
            logger.info("Getting all innings options")
            all_innings = page_nav.get_all_innings_options()
            print(f"    Found {len(all_innings)} innings: {all_innings}")
            logger.info(f"All innings: {all_innings}")

            # Identify regular innings and super overs
            regular_innings = [inn for inn in all_innings if 'Super Over' not in inn]
            super_overs = [inn for inn in all_innings if 'Super Over' in inn]

            logger.info(f"Regular innings: {regular_innings}")
            logger.info(f"Super overs: {super_overs}")

            # Set metadata for super overs
            metadata["has_super_over"] = len(super_overs) > 0
            metadata["super_over_count"] = len(super_overs)

            if super_overs:
                print(f"    ⚠ Match has {len(super_overs)} super over(s): {super_overs}")
                logger.info(f"Match has {len(super_overs)} super over(s)")

            # Set first and second innings metadata (regular innings only)
            if len(regular_innings) >= 2:
                if default_innings in regular_innings:
                    # Default is one of the regular innings
                    other_regular = [inn for inn in regular_innings if inn != default_innings][0]
                    metadata["first_innings"] = regular_innings[0]
                    metadata["second_innings"] = regular_innings[1]
                else:
                    # Default is super over, use regular innings in order
                    metadata["first_innings"] = regular_innings[0]
                    metadata["second_innings"] = regular_innings[1]
            elif len(regular_innings) == 1:
                metadata["first_innings"] = regular_innings[0]
                metadata["second_innings"] = None
            else:
                # No regular innings (shouldn't happen)
                metadata["first_innings"] = None
                metadata["second_innings"] = None

            # Scrape default innings first
            print(f"    Scraping {default_innings}...")
            logger.info(f"Scraping default innings: {default_innings}")

            default_html = driver.page_source
            default_data = CommentaryParser.parse_commentary(default_html)
            default_df = CommentaryParser.to_dataframe(default_data)
            default_df["innings"] = default_innings
            default_df["matchid"] = match_id
            default_df["is_super_over"] = 'Super Over' in default_innings  # Flag super over events

            print(f"    Extracted {len(default_df)} events from {default_innings}")
            logger.info(
                f"{default_innings}: {len(default_df)} events (super_over={default_df['is_super_over'].iloc[0] if len(default_df) > 0 else False})")

            # Store all innings dataframes
            all_innings_dfs = [default_df]

            # Scrape all other innings
            other_innings = [inn for inn in all_innings if inn != default_innings]

            for innings_name in other_innings:
                print(f"    Switching to {innings_name}...")
                logger.info(f"Switching to innings: {innings_name}")

                # Scroll to top before switch
                page_nav.scroll_to_top()
                time.sleep(3)

                # Switch to this innings
                page_nav.switch_to_innings(innings_name)
                print(f"    Switched to: {innings_name}")
                logger.info(f"Successfully switched to: {innings_name}")

                # Scroll to load commentary
                time.sleep(3)
                page_nav.scroll_full_page()
                time.sleep(3)

                # Extract commentary
                print(f"    Scraping {innings_name}...")
                logger.info(f"Scraping innings: {innings_name}")

                innings_html = driver.page_source
                innings_data = CommentaryParser.parse_commentary(innings_html)
                innings_df = CommentaryParser.to_dataframe(innings_data)
                innings_df["innings"] = innings_name
                innings_df["matchid"] = match_id
                innings_df["is_super_over"] = 'Super Over' in innings_name  # Flag super over events

                print(f"    Extracted {len(innings_df)} events from {innings_name}")
                logger.info(
                    f"{innings_name}: {len(innings_df)} events (super_over={innings_df['is_super_over'].iloc[0] if len(innings_df) > 0 else False})")

                all_innings_dfs.append(innings_df)

            # Combine all innings
            print(f"    Combining all innings...")
            logger.info("Combining all innings dataframes")
            final_df = pd.concat(all_innings_dfs, ignore_index=True)

            # Log summary
            total_events = len(final_df)
            super_over_events = final_df['is_super_over'].sum() if 'is_super_over' in final_df.columns else 0
            regular_events = total_events - super_over_events

            print(f"    Total events: {total_events} ({regular_events} regular + {super_over_events} super over)")
            logger.info(f"Total events: {total_events} across {len(all_innings_dfs)} innings")
            logger.info(f"  - Regular innings: {regular_events} events")
            logger.info(f"  - Super over(s): {super_over_events} events")

            # Clean column names
            final_df.columns = (
                final_df.columns
                .str.replace(r"[ ()]", "_", regex=True)
                .str.replace(r"_+", "_", regex=True)
                .str.strip("_")
                .str.lower()
            )

            # Convert metadata to DataFrame
            metadata_df = pd.DataFrame([metadata])

            # Clean column names
            metadata_df.columns = (
                metadata_df.columns
                .str.replace(r"[ ()]", "_", regex=True)
                .str.replace(r"_+", "_", regex=True)
                .str.strip("_")
                .str.lower()
            )


            # Save commentary to DB
            print(f"Saving commentary to database...")
            logger.info("Saving commentary to database")
            save_to_db("raw", "match_events", final_df)
            print(f"Commentary saved ({len(final_df)} events)")
            logger.info(f"Commentary saved: {len(final_df)} total events")

            # Save commentary to ADLS
            print(f"Saving metadata to ADLS...")
            logger.info("Saving metadata to ADLS")
            save_dataframe_to_adls(df=final_df, partition_key_column="matchid", file_format="csv",
                                   file_name="match_events")
            print(f"match_events saved to ADLS")
            logger.info(f"match_events saved to ADLS")

            # Save metadata to DB
            print(f"Saving metadata to database...")
            logger.info("Saving metadata to database")
            save_to_db("raw", "match_metadata", metadata_df)
            print(f"Metadata saved ({len(metadata_df)} rows)")
            logger.info(f"Metadata saved with super_over_count={metadata['super_over_count']}")

            # Save metadata to ADLS
            print(f"Saving metadata to ADLS...")
            logger.info("Saving metadata to ADLS")
            save_dataframe_to_adls(df = metadata_df,partition_key_column="matchid",file_format="json",file_name = "metadata")
            print(f"Metadata saved to ADLS")
            logger.info(f"Metadata saved to ADLS")

            # Save match_players to DB
            print(f"Saving match players to database...")
            logger.info("Saving match players to database")
            players_df.to_csv("players.csv")
            save_to_db("raw", "match_players", players_df)
            print(f"Match players saved ({len(players_df)} rows)")

            print(f"    ✓ Successfully scraped match {match_id}")
            logger.info(f"Successfully completed scraping match {match_id}")

            # Save players to ADLS
            print(f"Saving players to ADLS...")
            logger.info("Saving players to ADLS")
            save_dataframe_to_adls(df=players_df, partition_key_column="matchid", file_format="csv",
                                   file_name="match_players")
            print(f"players saved to ADLS")
            logger.info(f"players saved to ADLS")

        except TimeoutException as e:
            error_msg = f"Timeout error after {self.max_retries} retries: {str(e)[:200]}"
            print(f"    ✗ {error_msg}")
            logger.error(f"Timeout scraping match {match_id}: {e}")
            raise Exception(error_msg)

        except WebDriverException as e:
            error_msg = f"WebDriver error: {str(e)[:200]}"
            print(f"    ✗ {error_msg}")
            logger.error(f"WebDriver error scraping match {match_id}: {e}")
            raise Exception(error_msg)

        except Exception as e:
            error_msg = f"Unexpected error: {str(e)[:200]}"
            print(f"    ✗ {error_msg}")
            logger.error(f"Error scraping match {match_id}: {e}", exc_info=True)
            raise

        finally:
            # Always close driver
            if driver_manager:
                try:
                    print(f"    Closing WebDriver...")
                    logger.info("Closing WebDriver")
                    driver_manager.stop_driver()
                except Exception as e:
                    logger.warning(f"Error closing driver: {e}")

    def close(self):
        """Close and cleanup WebDriver"""
        if self.driver:
            try:
                logger.debug("Closing WebDriver...")
                self.driver.quit()
                logger.debug("WebDriver closed successfully")
            except Exception as e:
                logger.warning(f"Error closing WebDriver: {e}")
            finally:
                self.driver = None

    def __enter__(self):
        """Context manager entry"""
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        """Context manager exit - ensures cleanup"""
        self.close()
        return False

    def get_current_innings_team(self, driver):
        """Extract current innings team name from page"""
        try:
            # Use partial class matching for more flexibility
            team_element = driver.find_element(
                "css selector",
                "button.ds-capitalize.ds-cursor-pointer[class*='ds-border-color-border']"
            )
            team_name = team_element.text.strip()

            if not team_name:
                logger.warning("Team element found but text is empty")
                raise NoSuchElementException("Team name is empty")

            logger.debug(f"Extracted innings team: {team_name}")
            return team_name

        except NoSuchElementException as e:
            logger.error(f"Error extracting innings team: {e}")
            raise